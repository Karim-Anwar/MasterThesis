{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uhdpreeKNwIjlejjdzzz1DDfE-iHXYRX",
      "authorship_tag": "ABX9TyMMxTybEXKNpP8S/XiTYA57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karim-Anwar/MasterThesis/blob/main/AttemptFairViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNt1uVoHtIuZ",
        "outputId": "e549c7c2-16cb-4f04-952a-d05fab7b0687"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->timm)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "a1t7EoAivQSD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, annot_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annot_dir = annot_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load corresponding annotation file\n",
        "        annot_path = os.path.join(self.annot_dir, os.path.basename(img_path).replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt'))\n",
        "        with open(annot_path, 'r') as f:\n",
        "            annotation = f.read()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, annotation\n"
      ],
      "metadata": {
        "id": "bVQaVCF0t0BY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrackingDataset(Dataset):\n",
        "    def __init__(self, image_dir, annot_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annot_dir = annot_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load corresponding annotation file\n",
        "        annot_path = os.path.join(self.annot_dir, os.path.basename(img_path).replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt'))\n",
        "        with open(annot_path, 'r') as f:\n",
        "            annotation = f.read().strip().split()\n",
        "\n",
        "        class_label = int(annotation[0])  # Always 0 in this case\n",
        "        identity = int(annotation[1])\n",
        "        x_center = float(annotation[2])\n",
        "        y_center = float(annotation[3])\n",
        "        width = float(annotation[4])\n",
        "        height = float(annotation[5])\n",
        "\n",
        "        bbox = [x_center, y_center, width, height]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'class_label': class_label,\n",
        "            'identity': identity,\n",
        "            'bbox': torch.tensor(bbox)\n",
        "        }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "s1FYkJKfxxmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = CustomTrackingDataset(image_dir='/content/drive/MyDrive/sanity/train', annot_dir='/content/drive/MyDrive/sanity/train/annots', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = CustomTrackingDataset(image_dir='/content/drive/MyDrive/sanity/val', annot_dir='/content/drive/MyDrive/sanity/val/annots', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "i1bqozvyx145"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FairMOTViT(nn.Module):\n",
        "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=1, reid_dim=128):\n",
        "        super(FairMOTViT, self).__init__()\n",
        "        self.vit = create_model(vit_model_name, pretrained=True, num_classes=0)  # no classifier head\n",
        "        self.heatmap_head = nn.Conv2d(768, 1, kernel_size=3, padding=1)  # Heatmap for object detection\n",
        "        self.reid_head = nn.Conv2d(768, reid_dim, kernel_size=3, padding=1)  # Re-ID features\n",
        "        self.bbox_head = nn.Conv2d(768, 4, kernel_size=3, padding=1)  # Bounding box coordinates\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vit.forward_features(x)\n",
        "        B, N, C = features.shape\n",
        "        H = W = int(N**0.5)\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        heatmap = self.heatmap_head(features)\n",
        "        reid_features = self.reid_head(features)\n",
        "        bbox_regression = self.bbox_head(features)\n",
        "\n",
        "        return heatmap, reid_features, bbox_regression\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 1\n",
        "reid_dim = 128  # Dimension of Re-ID features\n",
        "model = FairMOTViT(num_classes=num_classes, reid_dim=reid_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "O3XKkYgACzhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tracking_loss(pred_heatmap, pred_reid_features, pred_bbox_regression,\n",
        "                  target_heatmap, target_reid_features, target_bbox):\n",
        "    # Heatmap loss (detection loss)\n",
        "    heatmap_loss = F.mse_loss(pred_heatmap, target_heatmap)\n",
        "\n",
        "    # Re-ID loss (triplet loss or contrastive loss can be used)\n",
        "    reid_loss = F.mse_loss(pred_reid_features, target_reid_features)\n",
        "\n",
        "    # Bounding box regression loss (smooth L1 loss)\n",
        "    bbox_regression_loss = F.smooth_l1_loss(pred_bbox_regression, target_bbox)\n",
        "\n",
        "    total_loss = heatmap_loss + reid_loss + bbox_regression_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "SlIsrNJDyLeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        inputs = sample['image'].to(device)\n",
        "        target_heatmap = sample['class_label'].float().to(device)\n",
        "        target_reid_features = sample['identity'].float().to(device)\n",
        "        target_bbox = sample['bbox'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        heatmap, reid_features, bbox_regression = model(inputs)\n",
        "\n",
        "        loss = tracking_loss(heatmap, reid_features, bbox_regression,\n",
        "                             target_heatmap, target_reid_features, target_bbox)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Epoch [{epoch}], Step [{batch_idx}], Loss: {loss.item():.4f}')\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for sample in val_loader:\n",
        "            inputs = sample['image'].to(device)\n",
        "            target_heatmap = sample['class_label'].float().to(device)\n",
        "            target_reid_features = sample['identity'].float().to(device)\n",
        "            target_bbox = sample['bbox'].to(device)\n",
        "\n",
        "            heatmap, reid_features, bbox_regression = model(inputs)\n",
        "\n",
        "            loss = tracking_loss(heatmap, reid_features, bbox_regression,\n",
        "                                 target_heatmap, target_reid_features, target_bbox)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Validation Loss: {val_loss:.4f}')\n",
        "    return val_loss\n",
        "\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "    val_loss = validate(model, device, val_loader)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "3H_tFyybyURR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aVsd9l9LtDEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm import create_model\n",
        "\n",
        "class CustomViT(nn.Module):\n",
        "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=1):\n",
        "        super(CustomViT, self).__init__()\n",
        "        self.vit = create_model(vit_model_name, pretrained=True, num_classes=num_classes)\n",
        "\n",
        "        # Define the 3x3 convolutional heads with 256 channels\n",
        "        self.heatmap_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.offset_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.size_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Define the re-ID convolutional layer with 128 kernels\n",
        "        self.reid_conv = nn.Conv2d(in_channels=768, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the ViT model\n",
        "        features = self.vit.forward_features(x)\n",
        "\n",
        "        # Reshape features to 2D spatial dimensions if needed (H, W)\n",
        "        B, N, C = features.shape\n",
        "        H = W = int(N**0.5)\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        # Apply each head to the features\n",
        "        heatmap = self.heatmap_head(features)\n",
        "        offset = self.offset_head(features)\n",
        "        size = self.size_head(features)\n",
        "\n",
        "        # Extract re-ID features\n",
        "        reid_features = self.reid_conv(features)\n",
        "\n",
        "        return heatmap, offset, size, reid_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "5DgLcBdztPXd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imape ="
      ],
      "metadata": {
        "id": "s0trovsNtMrP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}