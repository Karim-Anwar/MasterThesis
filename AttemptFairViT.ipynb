{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uhdpreeKNwIjlejjdzzz1DDfE-iHXYRX",
      "authorship_tag": "ABX9TyPpNSZ6RPo9Tji18PY/N8OV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karim-Anwar/MasterThesis/blob/main/AttemptFairViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from torchvision import transforms as T\n"
      ],
      "metadata": {
        "id": "mHE8OHXqdIAI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xyxy2xywh(x):\n",
        "    y = torch.zeros_like(x) if x.dtype is torch.float32 else np.zeros_like(x)\n",
        "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
        "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
        "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
        "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
        "    return y\n",
        "\n",
        "def letterbox(img, height=608, width=1088, color=(127.5, 127.5, 127.5)):\n",
        "    shape = img.shape[:2]\n",
        "    ratio = min(float(height) / shape[0], float(width) / shape[1])\n",
        "    new_shape = (round(shape[1] * ratio), round(shape[0] * ratio))\n",
        "    dw = (width - new_shape[0]) / 2\n",
        "    dh = (height - new_shape[1]) / 2\n",
        "    top, bottom = round(dh - 0.1), round(dh + 0.1)\n",
        "    left, right = round(dw - 0.1), round(dw + 0.1)\n",
        "    img = cv2.resize(img, new_shape, interpolation=cv2.INTER_AREA)\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "    return img, ratio, dw, dh\n",
        "\n",
        "def random_affine(img, targets=None, degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-2, 2), borderValue=(127.5, 127.5, 127.5)):\n",
        "    border = 0\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "\n",
        "    R = np.eye(3)\n",
        "    a = random.random() * (degrees[1] - degrees[0]) + degrees[0]\n",
        "    s = random.random() * (scale[1] - scale[0]) + scale[0]\n",
        "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n",
        "\n",
        "    T = np.eye(3)\n",
        "    T[0, 2] = (random.random() * 2 - 1) * translate[0] * img.shape[0] + border\n",
        "    T[1, 2] = (random.random() * 2 - 1) * translate[1] * img.shape[1] + border\n",
        "\n",
        "    S = np.eye(3)\n",
        "    S[0, 1] = math.tan((random.random() * (shear[1] - shear[0]) + shear[0]) * math.pi / 180)\n",
        "    S[1, 0] = math.tan((random.random() * (shear[1] - shear[0]) + shear[0]) * math.pi / 180)\n",
        "\n",
        "    M = S @ T @ R\n",
        "    imw = cv2.warpPerspective(img, M, dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=borderValue)\n",
        "\n",
        "    if targets is not None:\n",
        "        if len(targets) > 0:\n",
        "            n = targets.shape[0]\n",
        "            points = targets[:, 2:6].copy()\n",
        "            area0 = (points[:, 2] - points[:, 0]) * (points[:, 3] - points[:, 1])\n",
        "\n",
        "            xy = np.ones((n * 4, 3))\n",
        "            xy[:, :2] = points[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)\n",
        "            xy = (xy @ M.T)[:, :2].reshape(n, 8)\n",
        "\n",
        "            x = xy[:, [0, 2, 4, 6]]\n",
        "            y = xy[:, [1, 3, 5, 7]]\n",
        "            xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "\n",
        "            radians = a * math.pi / 180\n",
        "            reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n",
        "            x = (xy[:, 2] + xy[:, 0]) / 2\n",
        "            y = (xy[:, 3] + xy[:, 1]) / 2\n",
        "            w = (xy[:, 2] - xy[:, 0]) * reduction\n",
        "            h = (xy[:, 3] - xy[:, 1]) * reduction\n",
        "            xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n",
        "\n",
        "            np.clip(xy[:, 0], 0, width, out=xy[:, 0])\n",
        "            np.clip(xy[:, 2], 0, width, out=xy[:, 2])\n",
        "            np.clip(xy[:, 1], 0, height, out=xy[:, 1])\n",
        "            np.clip(xy[:, 3], 0, height, out=xy[:, 3])\n",
        "            w = xy[:, 2] - xy[:, 0]\n",
        "            h = xy[:, 3] - xy[:, 1]\n",
        "            area = w * h\n",
        "            ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))\n",
        "            i = (w > 4) & (h > 4) & (area / (area0 + 1e-16) > 0.1) & (ar < 10)\n",
        "\n",
        "            targets = targets[i]\n",
        "            targets[:, 2:6] = xy[i]\n",
        "\n",
        "        return imw, targets, M\n",
        "    else:\n",
        "        return imw\n",
        "\n",
        "class LoadImages:\n",
        "    def __init__(self, path, img_size=(1088, 608)):\n",
        "        if os.path.isdir(path):\n",
        "            image_format = ['.jpg', '.jpeg', '.png', '.tif']\n",
        "            self.files = sorted(glob.glob(f'{path}/*'))\n",
        "            self.files = list(filter(lambda x: os.path.splitext(x)[1].lower() in image_format, self.files))\n",
        "        elif os.path.isfile(path):\n",
        "            self.files = [path]\n",
        "        self.nF = len(self.files)\n",
        "        self.width = img_size[0]\n",
        "        self.height = img_size[1]\n",
        "        self.count = 0\n",
        "        assert self.nF > 0, f'No images found in {path}'\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if self.count == self.nF:\n",
        "            raise StopIteration\n",
        "        img_path = self.files[self.count]\n",
        "        img0 = cv2.imread(img_path)\n",
        "        assert img0 is not None, f'Failed to load {img_path}'\n",
        "        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "        img = np.ascontiguousarray(img, dtype=np.float32)\n",
        "        img /= 255.0\n",
        "        return img_path, img, img0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx % self.nF\n",
        "        img_path = self.files[idx]\n",
        "        img0 = cv2.imread(img_path)\n",
        "        assert img0 is not None, f'Failed to load {img_path}'\n",
        "        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "        img = np.ascontiguousarray(img, dtype=np.float32)\n",
        "        img /= 255.0\n",
        "        return img_path, img, img0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nF\n",
        "\n",
        "class LoadVideo:\n",
        "    def __init__(self, path, img_size=(1088, 608)):\n",
        "        if not os.path.isfile(path):\n",
        "            raise FileExistsError\n",
        "        self.cap = cv2.VideoCapture(path)\n",
        "        self.frame_rate = int(round(self.cap.get(cv2.CAP_PROP_FPS)))\n",
        "        self.vw = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        self.vh = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        self.vn = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        self.width = img_size[0]\n",
        "        self.height = img_size[1]\n",
        "        self.count = 0\n",
        "        self.w, self.h = self.get_size(self.vw, self.vh, self.width, self.height)\n",
        "        print(f'Length of the video: {self.vn} frames')\n",
        "\n",
        "    def get_size(self, vw, vh, dw, dh):\n",
        "        wa, ha = float(dw) / vw, float(dh) / vh\n",
        "        a = min(wa, ha)\n",
        "        return int(vw * a), int(vh * a)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if self.count == self.vn:\n",
        "            raise StopIteration\n",
        "        res, img0 = self.cap.read()\n",
        "        assert img0 is not None, f'Failed to load frame {self.count}'\n",
        "        img0 = cv2.resize(img0, (self.w, self.h))\n",
        "        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "        img = np.ascontiguousarray(img, dtype=np.float32)\n",
        "        img /= 255.0\n",
        "        return self.count, img, img0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vn\n",
        "\n",
        "class LoadImagesAndLabels:\n",
        "    def __init__(self, path, img_size=(1920, 1080), augment=False, transforms=None):\n",
        "        with open(path, 'r') as file:\n",
        "            self.img_files = file.readlines()\n",
        "            self.img_files = [x.replace('\\n', '') for x in self.img_files]\n",
        "            self.img_files = list(filter(lambda x: len(x) > 0, self.img_files))\n",
        "        self.label_files = [x.replace('images', 'labels_with_ids').replace('.png', '.txt').replace('.jpg', '.txt')\n",
        "                            for x in self.img_files]\n",
        "        self.nF = len(self.img_files)\n",
        "        self.width = img_size[0]\n",
        "        self.height = img_size[1]\n",
        "        self.augment = augment\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, files_index):\n",
        "        img_path = self.img_files[files_index]\n",
        "        label_path = self.label_files[files_index]\n",
        "        return self.get_data(img_path, label_path)\n",
        "\n",
        "    def get_data(self, img_path, label_path):\n",
        "        height = self.height\n",
        "        width = self.width\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f'File corrupt {img_path}')\n",
        "        augment_hsv = True\n",
        "        if self.augment and augment_hsv:\n",
        "            fraction = 0.50\n",
        "            img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "            S = img_hsv[:, :, 1].astype(np.float32)\n",
        "            V = img_hsv[:, :, 2].astype(np.float32)\n",
        "            a = (random.random() * 2 - 1) * fraction + 1\n",
        "            S *= a\n",
        "            if a > 1:\n",
        "                np.clip(S, a_min=0, a_max=255, out=S)\n",
        "            a = (random.random() * 2 - 1) * fraction + 1\n",
        "            V *= a\n",
        "            if a > 1:\n",
        "                np.clip(V, a_min=0, a_max=255, out=V)\n",
        "            img_hsv[:, :, 1] = S.astype(np.uint8)\n",
        "            img_hsv[:, :, 2] = V.astype(np.uint8)\n",
        "            cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "        img, ratio, padw, padh = letterbox(img, height=height, width=width)\n",
        "        if os.path.isfile(label_path):\n",
        "            labels0 = np.loadtxt(label_path, dtype=np.float32).reshape(-1, 6)\n",
        "            labels = labels0.copy()\n",
        "            labels[:, 2] = ratio * w * (labels0[:, 2] - labels0[:, 4] / 2) + padw\n",
        "            labels[:, 3] = ratio * h * (labels0[:, 3] - labels0[:, 5] / 2) + padh\n",
        "            labels[:, 4] = ratio * w * (labels0[:, 2] + labels0[:, 4] / 2) + padw\n",
        "            labels[:, 5] = ratio * h * (labels0[:, 3] + labels0[:, 5] / 2) + padh\n",
        "        else:\n",
        "            labels = np.array([])\n",
        "\n",
        "        if self.augment:\n",
        "            img, labels, M = random_affine(img, labels, degrees=(-5, 5), translate=(0.10, 0.10), scale=(0.50, 1.20))\n",
        "        nL = len(labels)\n",
        "        if nL > 0:\n",
        "            labels[:, 2:6] = xyxy2xywh(labels[:, 2:6].copy())\n",
        "            labels[:, 2] /= width\n",
        "            labels[:, 3] /= height\n",
        "            labels[:, 4] /= width\n",
        "            labels[:, 5] /= height\n",
        "        if self.augment:\n",
        "            lr_flip = True\n",
        "            if lr_flip & (random.random() > 0.5):\n",
        "                img = np.fliplr(img)\n",
        "                if nL > 0:\n",
        "                    labels[:, 2] = 1 - labels[:, 2]\n",
        "        img = np.ascontiguousarray(img[:, :, ::-1])\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return torch.tensor(img, dtype=torch.float32), labels, img_path, (h, w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nF\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, labels, paths, sizes = zip(*batch)\n",
        "    batch_size = len(labels)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    max_box_len = max([l.shape[0] for l in labels])\n",
        "    labels = [torch.from_numpy(l) for l in labels]\n",
        "    filled_labels = torch.zeros(batch_size, max_box_len, 6)\n",
        "    labels_len = torch.zeros(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        isize = labels[i].shape[0]\n",
        "        if len(labels[i]) > 0:\n",
        "            filled_labels[i, :isize, :] = labels[i]\n",
        "        labels_len[i] = isize\n",
        "    return imgs, filled_labels, paths, sizes, labels_len.unsqueeze(1)\n",
        "\n",
        "class JointDataset(LoadImagesAndLabels):\n",
        "    def __init__(self, root, paths, img_size=(1088, 608), augment=False, transforms=None):\n",
        "        dataset_names = paths.keys()\n",
        "        self.img_files = OrderedDict()\n",
        "        self.label_files = OrderedDict()\n",
        "        self.tid_num = OrderedDict()\n",
        "        self.tid_start_index = OrderedDict()\n",
        "        for ds, path in paths.items():\n",
        "            with open(path, 'r') as file:\n",
        "                self.img_files[ds] = file.readlines()\n",
        "                self.img_files[ds] = [os.path.join(root, x.strip()) for x in self.img_files[ds]]\n",
        "                self.img_files[ds] = list(filter(lambda x: len(x) > 0, self.img_files[ds]))\n",
        "            self.label_files[ds] = [x.replace('images', 'labels_with_ids').replace('.png', '.txt').replace('.jpg', '.txt').replace('.jpeg', '.txt')\n",
        "                                    for x in self.img_files[ds]]\n",
        "\n",
        "        for ds, label_paths in self.label_files.items():\n",
        "            max_index = -1\n",
        "            for lp in label_paths:\n",
        "                lb = np.loadtxt(lp)\n",
        "                if len(lb) < 1:\n",
        "                    continue\n",
        "                if len(lb.shape) < 2:\n",
        "                    img_max = lb[1]\n",
        "                else:\n",
        "                    img_max = np.max(lb[:, 1])\n",
        "                if img_max > max_index:\n",
        "                    max_index = img_max\n",
        "            self.tid_num[ds] = max_index + 1\n",
        "\n",
        "        last_index = 0\n",
        "        for i, (k, v) in enumerate(self.tid_num.items()):\n",
        "            self.tid_start_index[k] = last_index\n",
        "            last_index += v\n",
        "        self.nID = int(last_index + 1)\n",
        "        self.nds = [len(x) for x in self.img_files.values()]\n",
        "        self.cds = [sum(self.nds[:i]) for i in range(len(self.nds))]\n",
        "        self.nF = sum(self.nds)\n",
        "        self.width = img_size[0]\n",
        "        self.height = img_size[1]\n",
        "        self.augment = augment\n",
        "        self.transforms = transforms\n",
        "        print('=' * 80)\n",
        "        print('Dataset summary')\n",
        "        print(self.tid_num)\n",
        "        print('Total # identities:', self.nID)\n",
        "        print('Start index')\n",
        "        print(self.tid_start_index)\n",
        "        print('=' * 80)\n",
        "\n",
        "    def __getitem__(self, files_index):\n",
        "        for i, c in enumerate(self.cds):\n",
        "            if files_index >= c:\n",
        "                ds = list(self.label_files.keys())[i]\n",
        "                start_index = c\n",
        "        img_path = self.img_files[ds][files_index - start_index]\n",
        "        label_path = self.label_files[ds][files_index - start_index]\n",
        "        imgs, labels, img_path, (h, w) = self.get_data(img_path, label_path)\n",
        "        for i, _ in enumerate(labels):\n",
        "            if labels[i, 1] > -1:\n",
        "                labels[i, 1] += self.tid_start_index[ds]\n",
        "        return imgs, labels, img_path, (h, w)\n"
      ],
      "metadata": {
        "id": "GOlTW2LEdFvH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA"
      ],
      "metadata": {
        "id": "4K1UHF5Z97_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import json"
      ],
      "metadata": {
        "id": "l5_FxRBDHXWx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# directory = \"/content/drive/MyDrive/sanity/images\"\n",
        "# counter = 1\n",
        "\n",
        "# for root, dirs, files in os.walk(directory):\n",
        "#     counter = 1\n",
        "#     for file in files:\n",
        "#         if file.startswith(\"frame_\"):\n",
        "#             image_path = os.path.join(root, file)\n",
        "#             new_name = f\"{os.path.basename(root)}_{counter}.jpeg\"\n",
        "#             new_path = os.path.join(root, new_name)\n",
        "#             os.rename(image_path, new_path)\n",
        "#             counter += 1\n"
      ],
      "metadata": {
        "id": "RIGDbKZa9owb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import csv\n",
        "\n",
        "# root_directory = \"/content/drive/MyDrive/sanity/images\"\n",
        "\n",
        "# with open(\"sanity.train\", \"w\", newline=\"\") as train_file:\n",
        "#     writer = csv.writer(train_file)\n",
        "\n",
        "#     for dirpath, dirnames, filenames in os.walk(root_directory):\n",
        "#         for filename in filenames:\n",
        "#             base_name, extension = os.path.splitext(filename)\n",
        "#             if extension.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "#               image_path = os.path.join(dirpath, filename)\n",
        "#               writer.writerow([image_path])\n"
      ],
      "metadata": {
        "id": "8uCEM6qFuB3t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure run\n",
        "f = open('/content/drive/MyDrive/sanity/images/data_sanity.json')\n",
        "data_config = json.load(f)\n",
        "trainset_paths = data_config['train']\n",
        "dataset_root = data_config['root']\n",
        "f.close()\n",
        "\n",
        "\n",
        "# Initialize datasets and dataloaders\n",
        "dataset = JointDataset('/content/drive/MyDrive/sanity', paths=trainset_paths, img_size=(224, 224), augment=False, transforms=None)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "K3v_MOqxs92q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def visualize_image(dataset, idx, plot_width=10, plot_height=10):\n",
        "#     # Load image and annotations\n",
        "#     img, labels, img_path, (h, w) = dataset[idx]\n",
        "\n",
        "#     img = img\n",
        "\n",
        "#     # Plot the image\n",
        "#     fig, ax = plt.subplots(1, figsize=(plot_width, plot_height))\n",
        "#     ax.imshow(img)\n",
        "\n",
        "#     # Plot each bounding box\n",
        "#     for label in labels:\n",
        "#         if label.sum() == 0:  # Skip empty labels\n",
        "#             continue\n",
        "#         class_id, obj_id, x_center, y_center, width, height = label\n",
        "#         x_min = int((x_center - width / 2) * img.shape[1])\n",
        "#         y_min = int((y_center - height / 2) * img.shape[0])\n",
        "#         x_max = int((x_center + width / 2) * img.shape[1])\n",
        "#         y_max = int((y_center + height / 2) * img.shape[0])\n",
        "#         rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, edgecolor='yellow', facecolor='none', linewidth=1)\n",
        "#         ax.add_patch(rect)\n",
        "#         plt.text(x_min, y_min - 10, f'ID: {int(obj_id)}', color='red', fontsize=5, backgroundcolor=\"none\")\n",
        "\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "Vjcu4a99gVAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_idx = 200  # Change this to visualize different images\n",
        "# visualize_image(dataset, sample_idx, 12, 8)"
      ],
      "metadata": {
        "id": "aL844hQzb7SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL"
      ],
      "metadata": {
        "id": "-PHUSlyO93Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# import torch\n",
        "\n",
        "# class ViTBackbone(nn.Module):\n",
        "#     def __init__(self, embed_dim=768, num_heads=12, num_layers=12, num_classes=1):\n",
        "#         super(ViTBackbone, self).__init__()\n",
        "#         self.embed_dim = embed_dim\n",
        "#         self.num_heads = num_heads\n",
        "#         self.num_layers = num_layers\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "#         self.pos_embed = nn.Parameter(torch.zeros(1, 1 + 1, embed_dim))\n",
        "#         self.pos_drop = nn.Dropout(p=0.1)\n",
        "\n",
        "#         self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=16, stride=16)\n",
        "#         self.transformer = nn.Transformer(embed_dim, num_heads, num_layers)\n",
        "#         self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B = x.size(0)\n",
        "#         x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
        "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "#         x = torch.cat((cls_tokens, x), dim=1)\n",
        "#         x = self.pos_drop(x + self.pos_embed)\n",
        "\n",
        "#         x = self.transformer(x)\n",
        "#         x = self.head(x[:, 0])\n",
        "#         return x\n",
        "\n",
        "# class DetectionHead(nn.Module):\n",
        "#     def __init__(self, in_channels, num_classes):\n",
        "#         super(DetectionHead, self).__init__()\n",
        "#         self.heatmap_head = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "#         )\n",
        "#         self.offset_head = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(256, 2, kernel_size=1)\n",
        "#         )\n",
        "#         self.size_head = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(256, 2, kernel_size=1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         heatmap = self.heatmap_head(x)\n",
        "#         offset = self.offset_head(x)\n",
        "#         size = self.size_head(x)\n",
        "#         return heatmap, offset, size\n",
        "\n",
        "# class ReIDHead(nn.Module):\n",
        "#     def __init__(self, in_channels, embed_dim=128):\n",
        "#         super(ReIDHead, self).__init__()\n",
        "#         self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.conv(x)\n",
        "\n",
        "# class CustomModel(nn.Module):\n",
        "#     def __init__(self, num_classes=1):\n",
        "#         super(CustomModel, self).__init__()\n",
        "#         self.backbone = ViTBackbone()\n",
        "#         self.det_head = DetectionHead(self.backbone.embed_dim, num_classes)\n",
        "#         self.reid_head = ReIDHead(self.backbone.embed_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = self.backbone(x)\n",
        "#         heatmap, offset, size = self.det_head(features)\n",
        "#         reid_features = self.reid_head(features)\n",
        "#         return heatmap, offset, size, reid_features\n"
      ],
      "metadata": {
        "id": "vvaUU4Ndka2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "st5dKQOM0nRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "EbdV_Ldd0mAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(mask, 'b j -> b () () j')\n",
        "            dots.masked_fill_(~mask, mask_value)\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))\n",
        "            ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask = mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
        "        x = x.flatten(2)  # (B, embed_dim, N)\n",
        "        x = x.transpose(1, 2)  # (B, N, embed_dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_layers, embed_dim, num_heads, mlp_dim, channels=3, dropout=0.1, emb_dropout=0.1):\n",
        "        super(ViTBackbone, self).__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels=3, embed_dim=embed_dim)\n",
        "        # nn.Sequential(\n",
        "        #     Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "        #     nn.Linear(patch_dim, embed_dim)\n",
        "        # )\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(embed_dim, num_layers, num_heads, embed_dim // num_heads, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n+1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(DetectionHead, self).__init__()\n",
        "        self.heatmap_head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        )\n",
        "        self.offset_head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 2, kernel_size=1)\n",
        "        )\n",
        "        self.size_head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 2, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        heatmap = self.heatmap_head(x)\n",
        "        offset = self.offset_head(x)\n",
        "        size = self.size_head(x)\n",
        "        return heatmap, offset, size\n",
        "\n",
        "class ReIDHead(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim=128):\n",
        "        super(ReIDHead, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_layers, embed_dim, num_heads, mlp_dim, num_classes=1, channels=3, dropout=0.1, emb_dropout=0.1):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.backbone = ViTBackbone(image_size, patch_size, num_layers, embed_dim, num_heads, mlp_dim, channels, dropout, emb_dropout)\n",
        "        self.det_head = DetectionHead(embed_dim, num_classes)\n",
        "        self.reid_head = ReIDHead(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        B, N, C = features.shape\n",
        "        patch_dim = int((N - 1) ** 0.5)  # Number of patches per dimension\n",
        "        features = features[:, 1:].transpose(1, 2).reshape(B, C, patch_dim, patch_dim)\n",
        "        heatmap, offset, size = self.det_head(features)\n",
        "        reid_features = self.reid_head(features)\n",
        "        return heatmap, offset, size, reid_features"
      ],
      "metadata": {
        "id": "dPevw39K0kGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_radius(det_size, min_overlap=0.7):\n",
        "    height, width = det_size\n",
        "\n",
        "    a1 = 1\n",
        "    b1 = (height + width)\n",
        "    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
        "    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
        "    r1 = (b1 + sq1) / 2\n",
        "\n",
        "    a2 = 4\n",
        "    b2 = 2 * (height + width)\n",
        "    c2 = (1 - min_overlap) * width * height\n",
        "    sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
        "    r2 = (b2 + sq2) / 2\n",
        "\n",
        "    a3 = 4 * min_overlap\n",
        "    b3 = 2 * min_overlap * (height + width)\n",
        "    c3 = (min_overlap - 1) * width * height\n",
        "    sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
        "    r3 = (b3 + sq3) / 2\n",
        "\n",
        "    return min(r1, r2, r3)\n",
        "\n",
        "def draw_umich_gaussian(heatmap, center, radius, k=1):\n",
        "    diameter = 2 * radius + 1\n",
        "    gaussian = np.zeros((diameter, diameter), dtype=np.float32)\n",
        "    gaussian = cv2.getGaussianKernel(diameter, diameter / 6)\n",
        "    gaussian = gaussian * gaussian.T\n",
        "\n",
        "    x, y = center\n",
        "\n",
        "    height, width = heatmap.shape[:2]\n",
        "\n",
        "    left, right = min(x, radius), min(width - x, radius + 1)\n",
        "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "\n",
        "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
        "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
        "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n",
        "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
        "    return heatmap"
      ],
      "metadata": {
        "id": "zddPSoaq6aK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target_heatmap(bboxes, output_size, sigma=1):\n",
        "    \"\"\"\n",
        "    Compute the target heatmap.\n",
        "    Args:\n",
        "        bboxes (torch.Tensor): Bounding boxes, shape (N, 4).\n",
        "        output_size (tuple): Size of the output heatmap, (H, W).\n",
        "        sigma (float): Standard deviation for the Gaussian kernel.\n",
        "    Returns:\n",
        "        torch.Tensor: Target heatmap, shape (H, W).\n",
        "    \"\"\"\n",
        "    height, width = output_size\n",
        "    target_heatmap = torch.zeros(height, width)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        x_center, y_center, w, h = bbox\n",
        "        x_center = int(x_center * width)\n",
        "        y_center = int(y_center * height)\n",
        "\n",
        "        temp = torch.zeros(height, width)\n",
        "        temp[y_center, x_center] = 1\n",
        "        temp = torch.nn.functional.gaussian_blur(temp.unsqueeze(0).unsqueeze(0), (sigma, sigma), sigma)\n",
        "        target_heatmap = torch.max(target_heatmap, temp.squeeze(0).squeeze(0))\n",
        "\n",
        "    return target_heatmap\n",
        "\n",
        "def compute_target_offset(bboxes, output_size):\n",
        "    \"\"\"\n",
        "    Compute the target offset.\n",
        "    Args:\n",
        "        bboxes (torch.Tensor): Bounding boxes, shape (N, 4).\n",
        "        output_size (tuple): Size of the output offset map, (H, W).\n",
        "    Returns:\n",
        "        torch.Tensor: Target offset, shape (H, W, 2).\n",
        "    \"\"\"\n",
        "    height, width = output_size\n",
        "    target_offset = torch.zeros(height, width, 2)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        x_center, y_center, w, h = bbox\n",
        "        x_center = int(x_center * width)\n",
        "        y_center = int(y_center * height)\n",
        "\n",
        "        for y in range(height):\n",
        "            for x in range(width):\n",
        "                target_offset[y, x, 0] = x_center - x\n",
        "                target_offset[y, x, 1] = y_center - y\n",
        "\n",
        "    return target_offset\n",
        "\n",
        "\n",
        "def compute_target_size(bboxes, output_size):\n",
        "    \"\"\"\n",
        "    Compute the target size.\n",
        "    Args:\n",
        "        bboxes (torch.Tensor): Bounding boxes, shape (N, 4).\n",
        "        output_size (tuple): Size of the output size map, (H, W).\n",
        "    Returns:\n",
        "        torch.Tensor: Target size, shape (H, W, 2).\n",
        "    \"\"\"\n",
        "    height, width = output_size\n",
        "    target_size = torch.zeros(height, width, 2)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        x_center, y_center, w, h = bbox\n",
        "        x_center = int(x_center * width)\n",
        "        y_center = int(y_center * height)\n",
        "        w = int(w * width)\n",
        "        h = int(h * height)\n",
        "\n",
        "        x1, x2 = max(0, x_center - w // 2), min(width, x_center + w // 2)\n",
        "        y1, y2 = max(0, y_center - h // 2), min(height, y_center + h // 2)\n",
        "\n",
        "        for y in range(y1, y2):\n",
        "            for x in range(x1, x2):\n",
        "                target_size[y, x, 0] = w\n",
        "                target_size[y, x, 1] = h\n",
        "\n",
        "    return target_size\n"
      ],
      "metadata": {
        "id": "3kHPqKJj6Vmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define custom loss functions\n",
        "def heatmap_loss(pred_heatmap, target_heatmap):\n",
        "    return F.mse_loss(pred_heatmap, target_heatmap)\n",
        "\n",
        "def offset_loss(pred_offset, target_offset):\n",
        "    return F.smooth_l1_loss(pred_offset, target_offset)\n",
        "\n",
        "def size_loss(pred_size, target_size):\n",
        "    return F.smooth_l1_loss(pred_size, target_size)\n",
        "\n",
        "def reid_loss(pred_reid, target_reid):\n",
        "    return F.mse_loss(pred_reid, target_reid)\n",
        "\n",
        "def joint_loss(pred_heatmap, pred_offset, pred_size, pred_reid, target_heatmap, target_offset, target_size, target_reid):\n",
        "    return (heatmap_loss(pred_heatmap, target_heatmap) +\n",
        "            offset_loss(pred_offset, target_offset) +\n",
        "            size_loss(pred_size, target_size) +\n",
        "            reid_loss(pred_reid, target_reid))\n",
        "\n"
      ],
      "metadata": {
        "id": "R4a0EhKwmMqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, filled_labels, paths, sizes, labels_len) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch}\")):\n",
        "        # Transpose images to shape [batch_size, channels, height, width]\n",
        "        images = images.permute(0, 3, 1, 2).to(device)\n",
        "        filled_labels = filled_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_heatmap, pred_offset, pred_size, pred_reid = model(images)\n",
        "\n",
        "        # Extract targets from filled_labels\n",
        "        bboxes = filled_labels[..., 2:6]\n",
        "        embeddings = filled_labels[..., 1]\n",
        "\n",
        "        output_size = (images.shape[2] // 4, images.shape[3] // 4)  # Assuming the output feature map is 1/4 the input size\n",
        "\n",
        "        target_heatmap = compute_target_heatmap(bboxes, output_size)\n",
        "        target_offset = compute_target_offset(bboxes, output_size)\n",
        "        target_size = compute_target_size(bboxes)\n",
        "        target_reid = embeddings  # Assuming target re-id is the embeddings\n",
        "\n",
        "        target_heatmap = target_heatmap.to(device)\n",
        "        target_offset = target_offset.to(device)\n",
        "        target_size = target_size.to(device)\n",
        "        target_reid = target_reid.to(device)\n",
        "\n",
        "        loss = joint_loss(pred_heatmap, pred_offset, pred_size, pred_reid,\n",
        "                          target_heatmap, target_offset, target_size, target_reid)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(train_loader)\n"
      ],
      "metadata": {
        "id": "gA8xEEySzLiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, optimizer, and device\n",
        "model = CustomModel(image_size=224, patch_size=16, num_layers=12, embed_dim=768, num_heads=12, mlp_dim=768, num_classes=1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, device, dataloader, optimizer, epoch)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}')"
      ],
      "metadata": {
        "id": "G3TbXwm5mxh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appendix (Code Samples)"
      ],
      "metadata": {
        "id": "uYKnHqH03rfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "YcuNDxmf-Eem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for units in hidden_units:\n",
        "            layers.append(nn.Linear(units[0], units[1]))\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "BQ9PzrDwJ4sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
        "        x = x.flatten(2)  # (B, embed_dim, N)\n",
        "        x = x.transpose(1, 2)  # (B, N, embed_dim)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nROIk3z1-CpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, num_patches, embed_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed\n"
      ],
      "metadata": {
        "id": "xeuE89zD-Gy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wujLXV7M-PVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * mlp_ratio, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Gqno-jpp-ViD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTDetector(nn.Module):\n",
        "    def __init__(self, img_size=500, patch_size=16, num_classes=1, embed_dim=768, num_heads=12, mlp_ratio=4, depth=12, dropout_rate=0.1):\n",
        "        super(ViTDetector, self).__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels=3, embed_dim=embed_dim)\n",
        "        self.pos_embed = PositionalEncoding(self.num_patches, embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_rate) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            MLP(hidden_units=[(embed_dim, embed_dim * mlp_ratio), (embed_dim * mlp_ratio, embed_dim)], dropout_rate=dropout_rate),\n",
        "            nn.Linear(embed_dim, 4)  # output bounding box coordinates\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "model = ViTDetector()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "GUFHgN-1-ed0",
        "outputId": "183274b5-694a-4552-cc09-74274459f546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PatchEmbedding' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-413d088a7808>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViTDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-413d088a7808>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size, patch_size, num_classes, embed_dim, num_heads, mlp_ratio, depth, dropout_rate)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mViTDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         self.transformer_blocks = nn.ModuleList([\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PatchEmbedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-zbHwTwlVpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_loss(pred_bboxes, pred_class_logits, target_bboxes, target_labels):\n",
        "    # Bounding box regression loss\n",
        "    bbox_loss = F.smooth_l1_loss(pred_bboxes, target_bboxes, reduction='mean')\n",
        "\n",
        "    # Classification loss\n",
        "    class_loss = F.cross_entropy(pred_class_logits, target_labels, reduction='mean')\n",
        "\n",
        "    return bbox_loss + class_loss\n"
      ],
      "metadata": {
        "id": "BxC3Pt0aKHhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "ffl1Hn5PKS2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, bboxes, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch}\")):\n",
        "        images = torch.stack(images).to(device)\n",
        "        bboxes = torch.stack(bboxes).to(device)\n",
        "        labels = torch.stack(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_bboxes, pred_class_logits = model(images)\n",
        "        loss = criterion(pred_bboxes, pred_class_logits, bboxes, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, bboxes, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            images = torch.stack(images).to(device)\n",
        "            bboxes = torch.stack(bboxes).to(device)\n",
        "            labels = torch.stack(labels).to(device)\n",
        "\n",
        "            pred_bboxes, pred_class_logits = model(images)\n",
        "            loss = criterion(pred_bboxes, pred_class_logits, bboxes, labels)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, device, train_loader, detection_loss, optimizer, epoch)\n",
        "    val_loss = validate(model, device, val_loader, detection_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "vqMUk5ynKQEB",
        "outputId": "554ec3ee-4e78-49c6-c0b8-6d63ce1226de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 0:   0%|          | 0/520 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [11, 4] at entry 0 and [13, 4] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c33c44274ab0>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-c33c44274ab0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [11, 4] at entry 0 and [13, 4] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FairMOTViT(nn.Module):\n",
        "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=1, reid_dim=128):\n",
        "        super(FairMOTViT, self).__init__()\n",
        "        self.vit = create_model(vit_model_name, pretrained=True, num_classes=0)  # no classifier head\n",
        "        self.heatmap_head = nn.Conv2d(768, 1, kernel_size=3, padding=1)  # Heatmap for object detection\n",
        "        self.reid_head = nn.Conv2d(768, reid_dim, kernel_size=3, padding=1)  # Re-ID features\n",
        "        self.bbox_head = nn.Conv2d(768, 4, kernel_size=3, padding=1)  # Bounding box coordinates\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vit.forward_features(x)\n",
        "        B, N, C = features.shape\n",
        "        H = W = int(N**0.5)\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        heatmap = self.heatmap_head(features)\n",
        "        reid_features = self.reid_head(features)\n",
        "        bbox_regression = self.bbox_head(features)\n",
        "\n",
        "        return heatmap, reid_features, bbox_regression\n"
      ],
      "metadata": {
        "id": "PuWxO0om7Lxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FairMOTViT(nn.Module):\n",
        "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=1, reid_dim=128):\n",
        "        super(FairMOTViT, self).__init__()\n",
        "        self.vit = create_model(vit_model_name, pretrained=True, num_classes=0)  # no classifier head\n",
        "        self.heatmap_head = nn.Conv2d(768, 1, kernel_size=3, padding=1)  # Heatmap for object detection\n",
        "        self.reid_head = nn.Conv2d(768, reid_dim, kernel_size=3, padding=1)  # Re-ID features\n",
        "        self.bbox_head = nn.Conv2d(768, 4, kernel_size=3, padding=1)  # Bounding box coordinates\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vit.forward_features(x)\n",
        "        B, N, C = features.shape\n",
        "        H = W = int(N**0.5)\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        heatmap = self.heatmap_head(features)\n",
        "        reid_features = self.reid_head(features)\n",
        "        bbox_regression = self.bbox_head(features)\n",
        "\n",
        "        return heatmap, reid_features, bbox_regression\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 1\n",
        "reid_dim = 128  # Dimension of Re-ID features\n",
        "model = FairMOTViT(num_classes=num_classes, reid_dim=reid_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "O3XKkYgACzhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tracking_loss(pred_heatmap, pred_reid_features, pred_bbox_regression,\n",
        "                  target_heatmap, target_reid_features, target_bbox):\n",
        "    # Heatmap loss (detection loss)\n",
        "    heatmap_loss = F.mse_loss(pred_heatmap, target_heatmap)\n",
        "\n",
        "    # Re-ID loss (triplet loss or contrastive loss can be used)\n",
        "    reid_loss = F.mse_loss(pred_reid_features, target_reid_features)\n",
        "\n",
        "    # Bounding box regression loss (smooth L1 loss)\n",
        "    bbox_regression_loss = F.smooth_l1_loss(pred_bbox_regression, target_bbox)\n",
        "\n",
        "    total_loss = heatmap_loss + reid_loss + bbox_regression_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "SlIsrNJDyLeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        inputs = sample['image'].to(device)\n",
        "        target_heatmap = sample['class_label'].float().to(device)\n",
        "        target_reid_features = sample['identity'].float().to(device)\n",
        "        target_bbox = sample['bbox'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        heatmap, reid_features, bbox_regression = model(inputs)\n",
        "\n",
        "        loss = tracking_loss(heatmap, reid_features, bbox_regression,\n",
        "                             target_heatmap, target_reid_features, target_bbox)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Epoch [{epoch}], Step [{batch_idx}], Loss: {loss.item():.4f}')\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def validate(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for sample in val_loader:\n",
        "            inputs = sample['image'].to(device)\n",
        "            target_heatmap = sample['class_label'].float().to(device)\n",
        "            target_reid_features = sample['identity'].float().to(device)\n",
        "            target_bbox = sample['bbox'].to(device)\n",
        "\n",
        "            heatmap, reid_features, bbox_regression = model(inputs)\n",
        "\n",
        "            loss = tracking_loss(heatmap, reid_features, bbox_regression,\n",
        "                                 target_heatmap, target_reid_features, target_bbox)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Validation Loss: {val_loss:.4f}')\n",
        "    return val_loss\n",
        "\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "    val_loss = validate(model, device, val_loader)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "3H_tFyybyURR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVsd9l9LtDEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm import create_model\n",
        "\n",
        "class CustomViT(nn.Module):\n",
        "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=1):\n",
        "        super(CustomViT, self).__init__()\n",
        "        self.vit = create_model(vit_model_name, pretrained=True, num_classes=num_classes)\n",
        "\n",
        "        # Define the 3x3 convolutional heads with 256 channels\n",
        "        self.heatmap_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.offset_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.size_head = nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Define the re-ID convolutional layer with 128 kernels\n",
        "        self.reid_conv = nn.Conv2d(in_channels=768, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the ViT model\n",
        "        features = self.vit.forward_features(x)\n",
        "\n",
        "        # Reshape features to 2D spatial dimensions if needed (H, W)\n",
        "        B, N, C = features.shape\n",
        "        H = W = int(N**0.5)\n",
        "        features = features.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        # Apply each head to the features\n",
        "        heatmap = self.heatmap_head(features)\n",
        "        offset = self.offset_head(features)\n",
        "        size = self.size_head(features)\n",
        "\n",
        "        # Extract re-ID features\n",
        "        reid_features = self.reid_conv(features)\n",
        "\n",
        "        return heatmap, offset, size, reid_features\n",
        "\n",
        "\n"
      ]
    }
  ]
}